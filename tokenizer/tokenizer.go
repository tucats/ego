// Package tokenizer provides a language-aware tokenizer for Ego. This
// performs the functions of a lexical scanner, with the addition of doing
// parsing analysis of data types, which can set the token's class.
package tokenizer

import (
	"strconv"
	"strings"
	"text/scanner"
	"time"
	"unicode"

	"github.com/tucats/ego/app-cli/ui"
	"github.com/tucats/ego/util"
)

// Tokenizer is an instance of a tokenized string.
type Tokenizer struct {
	Source []string
	Tokens []Token
	TokenP int
	Line   []int
	Pos    []int
	silent bool
}

// EndOfTokens is a reserved token that means end of the buffer was reached.
var EndOfTokens = Token{class: EndOfTokensClass}

// ToTheEnd means to advance the token stream to the end.
const ToTheEnd = 999999

// This describes a token that is "crushed"; that is converting a sequence
// of tokens as generated by the standard Go scanner into a single Ego token
// where appropriate.
type crushedToken struct {
	source []Token
	result Token
}

// This is the table of tokens that are "crushed" into a single token.
var crushedTokens = []crushedToken{
	{
		source: []Token{AddToken, AssignToken},
		result: AddAssignToken,
	},
	{
		source: []Token{SubtractToken, AssignToken},
		result: SubtractAssignToken,
	},
	{
		source: []Token{MultiplyToken, AssignToken},
		result: MultiplyAssignToken,
	},
	{
		source: []Token{DivideToken, AssignToken},
		result: DivideAssignToken,
	},
	{
		source: []Token{AddToken, AddToken},
		result: IncrementToken,
	},
	{
		source: []Token{SubtractToken, SubtractToken},
		result: DecrementToken,
	},
	{
		source: []Token{InterfaceToken, DataBeginToken, DataEndToken},
		result: EmptyInterfaceToken,
	},
	{
		source: []Token{NewIdentifierToken(InterfaceToken.spelling), DataBeginToken, DataEndToken},
		result: EmptyInterfaceToken,
	},
	{
		source: []Token{BlockBeginToken, BlockEndToken},
		result: EmptyBlockToken,
	},
	{
		source: []Token{DotToken, DotToken, DotToken},
		result: VariadicToken,
	},
	{
		source: []Token{LessThanToken, SubtractToken},
		result: ChannelReceiveToken,
	},
	{
		source: []Token{GreaterThanToken, AssignToken},
		result: GreaterThanOrEqualsToken,
	},
	{
		source: []Token{LessThanToken, AssignToken},
		result: LessThanOrEqualsToken,
	},
	{
		source: []Token{AssignToken, AssignToken},
		result: EqualsToken,
	},
	{
		source: []Token{NotToken, AssignToken},
		result: NotEqualsToken,
	},
	{
		source: []Token{ColonToken, AssignToken},
		result: DefineToken,
	},
	{
		source: []Token{AndToken, AndToken},
		result: BooleanAndToken,
	},
	{
		source: []Token{OrToken, OrToken},
		result: BooleanOrToken,
	},
	{
		source: []Token{LessThanToken, LessThanToken},
		result: ShiftLeftToken,
	},
	{
		source: []Token{GreaterThanToken, GreaterThanToken},
		result: ShiftRightToken,
	},
}

// New creates a tokenizer instance and breaks the string
// up into an array of tokens. The isCode flag is used to
// indicate this is Ego code, which has some different
// tokenizing rules.
func New(src string, isCode bool) *Tokenizer {
	var (
		nextToken Token
		s         scanner.Scanner
	)

	start := time.Now()
	lines := splitLines(src, isCode)
	src = strings.Join(lines, "\n")
	t := Tokenizer{
		Source: lines,
		TokenP: 0,
		silent: true,
		Tokens: make([]Token, 0),
	}

	s.Init(strings.NewReader(src))
	s.Error = func(s *scanner.Scanner, msg string) { /* suppress messaging */ }
	s.Filename = "Input"

	for tok := s.Scan(); tok != scanner.EOF; tok = s.Scan() {
		nextTokenSpelling := s.TokenText()

		if TypeTokens[NewTypeToken(nextTokenSpelling)] {
			nextToken = NewTypeToken(nextTokenSpelling)
		} else if util.InList(nextTokenSpelling, "true", "false") {
			nextToken = Token{class: BooleanTokenClass, spelling: nextTokenSpelling}
		} else if tx := NewReservedToken(nextTokenSpelling); tx.IsReserved(true) {
			nextToken = tx
		} else if IsSymbol(nextTokenSpelling) {
			nextToken = NewIdentifierToken(nextTokenSpelling)
		} else if SpecialTokens[NewSpecialToken(nextTokenSpelling)] {
			nextToken = NewSpecialToken(nextTokenSpelling)
		} else if strings.HasPrefix(nextTokenSpelling, "\"") && strings.HasSuffix(nextTokenSpelling, "\"") {
			rawString := unQuote(nextTokenSpelling)
			nextToken = NewStringToken(rawString)
		} else if strings.HasPrefix(nextTokenSpelling, "`") && strings.HasSuffix(nextTokenSpelling, "`") {
			nextToken = NewStringToken(strings.TrimPrefix(strings.TrimSuffix(nextTokenSpelling, "`"), "`"))
		} else if _, err := strconv.ParseInt(nextTokenSpelling, 10, 64); err == nil {
			nextToken = Token{class: IntegerTokenClass, spelling: nextTokenSpelling}
		} else if _, err := strconv.ParseFloat(nextTokenSpelling, 64); err == nil {
			nextToken = Token{class: FloatTokenClass, spelling: nextTokenSpelling}
		} else {
			nextToken = Token{class: ValueTokenClass, spelling: nextTokenSpelling}
		}

		t.Tokens = append(t.Tokens, nextToken)
		column := s.Column

		// See if this is one of the special cases convert multiple tokens into
		// a single token?
		if isCode {
			for _, crush := range crushedTokens {
				if len(crush.source) > len(t.Tokens) {
					continue
				}

				found := true
				// See if the current token stream now ends with a sequence that should
				// be collapsed. If we look at each source token and never get a mismatch
				// we know this was still found.
				for i, ch := range crush.source {
					if t.Tokens[len(t.Tokens)-len(crush.source)+i] != ch {
						found = false

						break
					}
				}

				// If we found a match here, lop off the individual tokens
				// and replace the "current" token with the crushed value
				if found {
					t.Tokens = append(t.Tokens[:len(t.Tokens)-len(crush.source)], crush.result)

					// We also must adjust the Line and Pos arrays accordingly. Remove as many
					// items from the end as needed.
					t.Line = t.Line[:len(t.Line)-len(crush.source)+1]
					t.Pos = t.Pos[:len(t.Pos)-len(crush.source)+1]

					// Adjust the column to reflect the character position of the
					// start of the crushed token.
					column = column - len(crush.result.Spelling())

					break
				}
			}
		}

		// Add in the line from the scan and the (possibly adjusted) column
		t.Line = append(t.Line, s.Line)
		t.Pos = append(t.Pos, column)
	}

	if !t.silent && ui.IsActive(ui.TokenLogger) {
		t.DumpTokens()
	}

	ui.Log(ui.TokenLogger, "### Tokenization completed, %d tokens, %s", len(t.Tokens), time.Since(start))

	return &t
}

// EnableDump causes the token stream to be dumped to the console. This is only used for
// internnal debugging.
func (t *Tokenizer) EnableDump() {
	t.silent = false
}

// Construct a new token given a class and spelling.
func (t *Tokenizer) NewToken(class TokenClass, spelling string) Token {
	return Token{class: ValueTokenClass, spelling: spelling}
}

// IsSymbol is a utility function to determine if a string contains is a symbol name.
func IsSymbol(s string) bool {
	for n, c := range s {
		if c == '_' || unicode.IsLetter(c) {
			continue
		}

		if n > 0 && unicode.IsDigit(c) {
			continue
		}

		return false
	}

	return true
}

// GetTokens returns a string representing the tokens within the
// given range of tokens.
func (t *Tokenizer) GetTokens(pos1, pos2 int, spacing bool) string {
	var (
		s  strings.Builder
		p1 = pos1
		p2 = pos2
	)

	if p1 < 0 {
		p1 = 0
	} else if p1 > len(t.Tokens) {
		p1 = len(t.Tokens)
	}

	if p2 < p1 {
		p2 = p1
	} else {
		if p2 > len(t.Tokens) {
			p2 = len(t.Tokens)
		}
	}

	for _, t := range t.Tokens[p1:p2] {
		s.WriteString(t.spelling)

		if spacing {
			s.WriteRune(' ')
		}
	}

	return s.String()
}

// InList is a support function that checks to see if a string matches
// any of a list of other strings.
func InList(s Token, test ...Token) bool {
	for _, t := range test {
		if s == t {
			return true
		}
	}

	return false
}

// unQuote will remove quotes and also process any escapes in the string.
// It is a wrapper around strconv.Unquote but does not report an error if
// the string is improperly formed.
func unQuote(input string) string {
	result, err := strconv.Unquote(input)
	if err != nil {
		return input
	}

	return result
}

// Close discards any storage no longer needed by the tokenizer. The
// line number and position arrays as well as the source are maintained
// to support error reporting.
func (t *Tokenizer) Close() {
	// We no longer need the token array, so free up the memory.
	t.Tokens = nil
}
